<!DOCTYPE html>
<html lang="zxx">
<head>
    <meta charset="UTF-8" />
    <meta content="width=device-width,initial-scale=1.0,maximum-scale=1.0" name="viewport">
    <!-- Open Graph -->
    <meta property="og:title" content="Azure Databricks Best Practices | MAQ Software Insights" />
    <meta property="og:type" content="website" />
    <meta property="og:image" content="/images/expertise/cloud-main.jpg" />
    <meta property="og:description" content="Improve the speed and functionality of your Power BI reports. Optimize DAX syntax and queries and discover the 8 most common mistakes to avoid." />
    <!-- Twitter Theme -->
    <meta name="twitter:widgets:theme" content="light">

    <!-- Title &amp; Favicon -->
    <title>Azure Databricks Best Practices | MAQ Software Insights</title>
    <link rel="shortcut icon" type="image/x-icon" href="/images/logos/MAQ-Software-URL.png">
    <!-- Font -->
    <link href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CHind+Madurai:400,500&amp;subset=latin-ext" rel="stylesheet">

    <!-- Css -->
    <link rel="stylesheet" href="/css/core.min.css" />
    <link rel="stylesheet" href="/css/skin.css" />
    <!--[if lt IE 9]>
        <script type="text/javascript" src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
</head>
<body class="shop blog">

    <!-- Header -->
    <header id="header" class="header header-absolute header-fixed-on-mobile header-transparent" data-bkg-threshold="100" data-sticky-threshold="0"></header>
    <!-- Header End -->
    <div class="section-block bkg-grey-ultralight"></div>
    <!-- Content -->
    <div class="content clearfix">
        <div class="section-block clearfix pt-0 pb-0 bkg-grey-ultralight">
            <div class="row">
                <!-- Content Inner -->
                <div class="column width-10 offset-1 content-inner blog-single-post bkg-grey-ultralight">
                    <article class="post">
                        <div class="post-content with-background">
                            <h1 class="post-title center">Azure Databricks Best Practice Guide</h1>
                            <div class="post-info center">
                                <span class="post-date">Last updated: June 11, 2021 &nbsp;&nbsp;|&nbsp;&nbsp;</span><span class="post-date"></span>  Posted: October 3, 2020</span>
                            </div>
                            <p class="text-large">Azure Databricks (ADB) has the power to process terabytes of data, while simultaneously running heavy data science workloads. Over time, as data input and workloads increase, job performance decreases. As an ADB developer, optimizing your platform enables you to work faster and save hours of effort for you and your team. Below are the 18 best practices you need to optimize your ADB environment.</p>
                            <div id="security">
                                <ol>
                                    <li class="text-xlarge"><strong>Delete temporary tables after notebook execution</strong></li>
                                    <br>
                                    <p class="text-large">Delete temporary tables that were created as intermediate tables during notebook execution. Deleting tables saves storage, especially if the notebook is scheduled daily.</p>
                                    <li class="text-xlarge"><strong>Use <code class="color-red-light bkg-grey-ultralight">dbutils.fs.rm()</code> to permanently delete temporary table metadata</strong></li>
                                    <br>
                                    <p class="text-large">ADB clusters store table metadata, even if you use drop statements to delete. Before creating temporary tables, use <code class="color-red bkg-grey-ultralight">dbutils.fs.rm()</code> to permanently delete metadata. If you don’t use this statement, an error message will appear stating that the table already exists. To avoid this error in daily refreshes, you must use <code class="color-red bkg-grey-ultralight">dbutils.fs.rm()</code>.</p>
                                    <li class="text-xlarge"><strong>Use Lower() or Upper() when comparing strings to avoid losing data</strong></li>
                                    <br>
                                    <p class="text-large">ADB can't compare strings with different casing. To avoid losing data, use case conversion statements Lower() or Upper(). Example:</p>
                                    <p class="text-large"><strong><code class="color-red bkg-grey-ultralight">SELECT 'MAQSoftware' = 'maqsoftware' AS WithOutLowerOrUpper
                                        <br>,LOWER('MAQSoftware') = 'maqsoftware' AS WithLower
                                        <br>,UPPER('MAQSoftware') = 'MAQSOFTWARE' AS WithUpper</code></strong></p>
                                    <li class="text-xlarge"><strong>Use Scala for data processing</strong></li>
                                    <br>
                                    <p class="text-large">ADB’s data processing is based on Apache Spark, which is written in Scala. As a result, Scala performs better than SQL in ADB. Note: Python should still be used for machine learning functions in ADB.</p>
                                    <li class="text-xlarge"><strong>Use custom functions to simplify complex calculations</strong></li>
                                    <br>
                                    <p class="text-large">If your calculation requires multiple steps, you can save time and by creating a one-step custom function. ADB offers a variety of built in SQL functions, however to create custom functions, known as user-defined functions (UDF), use Scala. Once you have a custom function, you can call it every time you need to perform that specific calculation.</p>
                                    <li class="text-xlarge"><strong>Use Delta tables for DML commands</strong></li>
                                    <br>
                                    <p class="text-large">In ADB, Hive tables do not support UPDATE and MERGE statements or NOT NULL and CHECK constraints. Delta tables do support these commands, however running large amounts of data on Delta tables decreases query performance. So not to decrease performance, store table versions.</p>
                                    <li class="text-xlarge"><strong>Use views when creating intermediate tables</strong></li>
                                    <br>
                                    <p class="text-large">If you need to create intermediate tables, use views to minimize storage usage and save costs. Views are session-oriented and will automatically remove tables from storage after query execution. For optimal query performance, do not use joins or subqueries in views.</p>
                                    <li class="text-xlarge"><strong>Enable adaptive query execution (AQE)</strong></li>
                                    <br>
                                    <p class="text-large">AQE improves large query performance. By default, AQE is disabled in ADB. To enable it, use: <code class="color-red bkg-grey-ultralight">set spark.sql.adaptive.enabled = true;</code></p>
                                    <li class="text-xlarge"><strong>Partition by columns</strong></li>
                                    <br>
                                    <p class="text-large">Delta tables in ADB support partitioning, which enhances performance. You can partition by a column if you expect data in that partition to be at least 1 GB. If column cardinality is high, do not use that column for partitioning. For example, if you partition by user ID and there are 1M distinct user IDs, partitioning would increase table load time. Syntax example:
                                        <br>
                                        <code class="color-red bkg-grey-ultralight">
                                        <br>CREATE TABLE events (
                                        <br>DATE DATE
                                        <br>,eventId STRING
                                        <br>,eventType STRING
                                        <br>,data STRING
                                        <br>) USING delta PARTITIONED BY (DATE)</code>
                                    </p>
                                    <li class="text-xlarge"><strong>Use key vault credentials when creating mount points</strong></li>
                                    <br>
                                    <p class="text-large">When creating mount points to Azure Data Lake Storage (ADLS), use a key vault client ID and client secret to enhance security.</p>
                                    <li class="text-xlarge"><strong>Query directly on parquet files from ADLS</strong></li>
                                    <br>
                                    <p class="text-large">If you need to use the data from parquet files, do not extract into ADB in intermediate table format. Instead, directly query on the parquet file to save time and storage. Example:
                                        <br><code class="color-red bkg-grey-ultralight">SELECT ColumnName FROM parquet.`Location of the file`</code>
                                    </p>
                                    <li class="text-xlarge"><strong>Specify distribution when publishing data to Azure Data Warehouse (ADW)</strong></li>
                                    <br>
                                    <p class="text-large">Use hash distribution for fact tables or large tables, round robin for dimensional tables, replicated for small dimensional tables. Example:
                                        <br>
                                        <code class="color-red bkg-grey-ultralight">
                                        df.write \
                                        <br>.format("com.databricks.spark.sqldw") \
                                        <br>.option("url", "jdbc:sqlserver://<the-rest-of-the-connection-string>
                                        <br>") \
                                        <br>.option("forwardSparkAzureStorageCredentials", "true") \
                                        <br>.option("dbTable", "my_table_in_dw_copy") \
                                        <br>.option("tableOptions", "table_options") \
                                        <br>.save()</code>
                                    </p>
                                    <li class="text-xlarge"><strong>Customize cluster termination time</strong></li>
                                    <br>
                                    <p class="text-large">Terminating inactive clusters saves costs. ADB automatically terminates clusters based on a default down time. As different projects have different needs, it’s important to customize the down time to avoid premature or delayed termination. For example: set a longer down time for development environments, as work is continuous.</p>
                                    <li class="text-xlarge"><strong>Enable cluster autoscaling</strong></li>
                                    <br>
                                    <p class="text-large">ADB offers cluster autoscaling, which is disabled by default. Enable this feature to enhance job performance. Instead of providing a fixed number of worker nodes during cluster creation, you should provide a minimum and maximum. ADB then automatically reallocates the worker nodes based on job characteristics.</p>
                                    <li class="text-xlarge"><strong>Use Azure Data Factory (ADF) to run ADB notebook jobs</strong></li>
                                    <br>
                                    <p class="text-large">If you run numerous notebooks daily, the ADB job scheduler will not be efficient. The ADB job scheduler cannot set notebook dependency, so you would have to store all notebooks in one master, which is difficult to debug. Instead, schedule jobs through Azure Data Factory, which enables you to set dependency and easily debug if anything fails.</p>
                                    <li class="text-xlarge"><strong>Use the retry feature in ADF when scheduling jobs</strong></li>
                                    <br>
                                    <p class="text-large">Processing notebooks in ADB through ADF can overload the cluster, causing notebooks to fail. If failure occurs, the entire job should not stop. To continue work from the point of failure, set ADF to retry two to three times with five-minute intervals. As a result, the processing should continue from the set time, saving you time and effort.</p>
                                    <li class="text-xlarge"><strong>Implement failure checkpoints while publishing data</strong></li>
                                    <br>
                                    <p class="text-large">With ADB, you can dump data into multiple resources like ADW or ADLS. Publishing numerous tables to another resource takes time. If publishing fails, do not restart the entire process. Implement checkpoints, so that you can restart from the point of failure.</p>
                                    <li class="text-xlarge"><strong>Consider upgrading to ADB Premium</strong></li>
                                    <br>
                                    <p class="text-large">Your business’s data has never been more valuable. Additional security is a worthwhile investment. ADB Premium includes 5-level access control. For more features, check out: <a href="https://azure.microsoft.com/en-us/pricing/details/databricks/" target="blank" class="underlined">Premium vs. Standard</a>.</p>
                                </ol>
                            </div>

                            <h2>References</h2>
                            <ul>
                                <li class="text-large"><a href="https://docs.microsoft.com/en-us/azure/databricks/delta/">Delta tables</a> – Microsoft Corporation, last updated June 11, 2021</li>
                                <li class="text-large"><a href="https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/language-manual/sql-ref-datatypes#sql">Datatypes in ADB</a> – Microsoft Corporation, last updated June 29, 2021</li>
                                <li class="text-large"><a href="https://docs.microsoft.com/en-us/azure/databricks/scenarios/store-secrets-azure-key-vault">Key Vault usage</a> – Microsoft Corporation, last updated July 19, 2019</li>
                                <li class="text-large"><a href="https://docs.microsoft.com/en-us/azure/databricks/clusters/configure#cluster-size-and-autoscaling">Configure Clusters (Autoscaling)</a> – Microsoft Corporation, last updated August 9, 2021</li>
                                <li class="text-large"><a href="https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html">Features in AQE</a> – Databricks, last updated May 29, 2020</li>
                                <li class="text-large"><a href="https://docs.databricks.com/clusters/clusters-manage.html#automatic-termination-1">Automatic termination</a> – Databricks, last updated August 10, 2021</li>
                            </ul>
                            <div class="section-block"></div>
                            <div class="row">
                                <div class="column width-6">
                                    <img src="/images/best-practices/BPG004-main.jpg" alt="">
                                </div>
                                <div class="column width-5 offset-1">
                                    <p class="title-medium">Up Next</p><br><p class="title-small">Azure Optimization Best Practices</p><br><a href="/insights/azure-architecture-best-practices">Learn More &rarr;</a><br>
                                </div>
                            </div>
                        </div>
                    </article>
                </div>
            </div>
        </div>
        <div class="section-block bkg-grey-ultralight"></div>
    </div>
    <!-- Content End -->
    <!-- Footer -->
    <footer id="footer" class="footer footer-light bkg-grey-ultralight"></footer>
    <!-- Footer End -->
    <!-- Js -->
    <script src="/js/jquery-3.2.1.min.js"></script>
    <script src="/js/timber.master.min.js"></script>
    <script>
$("#header").load("/header");
$("#footer").load("/footer");
    </script>
</body>
</html>